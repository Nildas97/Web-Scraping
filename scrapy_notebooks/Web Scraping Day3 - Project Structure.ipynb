{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45466e9f",
   "metadata": {},
   "source": [
    "# PROJECT STRUCTURE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aed6281",
   "metadata": {},
   "source": [
    "### By - Nilutpal Das"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37822cf3",
   "metadata": {},
   "source": [
    "### 1. checking the current directory and list of directory inside the directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66d8740e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\Data Science\\\\jupyter-notebooks\\\\data science\\\\web_scraping'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba043241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive D is Data\n",
      " Volume Serial Number is EC2B-97AB\n",
      "\n",
      " Directory of D:\\Data Science\\jupyter-notebooks\\data science\\web_scraping\n",
      "\n",
      "27-12-2023  08.07 PM    <DIR>          .\n",
      "27-12-2023  08.07 PM    <DIR>          ..\n",
      "27-12-2023  08.05 PM    <DIR>          .ipynb_checkpoints\n",
      "27-12-2023  08.07 PM               589 Untitled.ipynb\n",
      "27-12-2023  07.10 PM             5,972 Web Scraping Day1 - Intro to web scraping and crawling.ipynb\n",
      "27-12-2023  08.03 PM             2,754 Web Scraping Day2 - Installation.ipynb\n",
      "26-12-2023  07.59 PM    <DIR>          webscrap\n",
      "27-12-2023  08.04 PM    <DIR>          webscrapy\n",
      "               3 File(s)          9,315 bytes\n",
      "               5 Dir(s)  418,632,015,872 bytes free\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e86fac6",
   "metadata": {},
   "source": [
    "### 2. the scrapy project directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ecd34a82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Data Science\\jupyter-notebooks\\data science\\web_scraping\\webscrapy\n"
     ]
    }
   ],
   "source": [
    "cd webscrapy/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65550cd",
   "metadata": {},
   "source": [
    "### 3. inside the project directory, there is another folder with same project directory name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23e460df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive D is Data\n",
      " Volume Serial Number is EC2B-97AB\n",
      "\n",
      " Directory of D:\\Data Science\\jupyter-notebooks\\data science\\web_scraping\\webscrapy\n",
      "\n",
      "27-12-2023  08.04 PM    <DIR>          .\n",
      "27-12-2023  08.04 PM    <DIR>          ..\n",
      "27-12-2023  08.04 PM               272 scrapy.cfg\n",
      "27-12-2023  08.04 PM    <DIR>          webscrapy\n",
      "               1 File(s)            272 bytes\n",
      "               3 Dir(s)  418,632,015,872 bytes free\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1995336",
   "metadata": {},
   "source": [
    "### 4. if you go inside the project directory, you will see list of files inside it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5fb712c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Data Science\\jupyter-notebooks\\data science\\web_scraping\\webscrapy\\webscrapy\n"
     ]
    }
   ],
   "source": [
    "cd webscrapy/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd4c7ea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive D is Data\n",
      " Volume Serial Number is EC2B-97AB\n",
      "\n",
      " Directory of D:\\Data Science\\jupyter-notebooks\\data science\\web_scraping\\webscrapy\\webscrapy\n",
      "\n",
      "27-12-2023  08.04 PM    <DIR>          .\n",
      "27-12-2023  08.04 PM    <DIR>          ..\n",
      "27-12-2023  07.18 PM                 0 __init__.py\n",
      "27-12-2023  08.04 PM               277 items.py\n",
      "27-12-2023  08.04 PM             3,757 middlewares.py\n",
      "27-12-2023  08.04 PM               376 pipelines.py\n",
      "27-12-2023  08.04 PM             3,410 settings.py\n",
      "27-12-2023  07.18 PM    <DIR>          spiders\n",
      "               5 File(s)          7,820 bytes\n",
      "               3 Dir(s)  418,632,015,872 bytes free\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d6dd62",
   "metadata": {},
   "source": [
    "## FILES LISTS <br>\n",
    "### 1. spiders folder - inside it has .init.py it is the file in which all the project code will be written."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c856c1e",
   "metadata": {},
   "source": [
    "### 2. settings.py - it has all the settings of project. it has numerous variables starting with : \n",
    "### a. bot_name, i.e the web scraping application name. <br> b. user_agent - your domain name. <br> c. robot_txt = it is in boolean. if it is true then it will follow all the web scraping related instructions created by the websites. <br> d. concurrent_requests - no. of time the application is sending requests to open the website. default is 16 or you can use 32 at max. more than that will create a lot of load onto the website."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81f1cbc",
   "metadata": {},
   "source": [
    "### 3. items.py - it stores all the item names that you are going to scrape from a website, be it product_name, product_description, product_price etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0659d2e5",
   "metadata": {},
   "source": [
    "### 4. pipelines.py - it handles all the process in the way to store the data that have been scraped be it in the CSV, JSON, or in SQL database or in MongoDB database. it structures the process of handling the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7268444a",
   "metadata": {},
   "source": [
    "### 5. middlewares.py - it is clearly explained through its name. it acts as a middle person in both situation while sending requests to websites using proxies (to bypass the restriction for webscraping). middlewares manages this portion. it also manages the requests getting back to machine from the websites be it to store it in database or in any format."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
