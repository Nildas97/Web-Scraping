{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8cdb51c4",
   "metadata": {},
   "source": [
    "# STORING THE DATA IN MongoDB DATABASE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a367ae",
   "metadata": {},
   "source": [
    "### By - Nilutpal Das"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc226622",
   "metadata": {},
   "source": [
    "# NOTE : the process shown in this section is built to be run in dedicated IDE also inside a specific folder discussed in the previous notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5622255",
   "metadata": {},
   "source": [
    "### Storing the data in MongoDB is lil bit different from storing the data in other databases. The main reason is that MySQL or PostgreSQL these all are called realational databases which have structured formatting techniques of storing data. the MongoDB way is different because it is non relational database so it has unstructured formatting techniques storing data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be0c480",
   "metadata": {},
   "source": [
    "### 1. the code in the scrapper program i.e. scrapyquotes.py will remain same. the modification need to be done will be in the pipelines.py file. so lets check the pipelines.py, and do the modifications needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc4021a",
   "metadata": {},
   "source": [
    "### as we know that data stored in MongoDB is in unstructured format, basically it stores the data in dictionary format or in json format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ea011c",
   "metadata": {},
   "source": [
    "## pipelines.py"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b145bd30",
   "metadata": {},
   "source": [
    "# Define your item pipelines here\n",
    "#\n",
    "# Don't forget to add your pipeline to the ITEM_PIPELINES setting\n",
    "# See: https://docs.scrapy.org/en/latest/topics/item-pipeline.html\n",
    "\n",
    "\n",
    "# useful for handling different item types with a single interface\n",
    "\n",
    "from .mongodb_client import get_mongodb_client\n",
    "import logging\n",
    "\n",
    "class WebscrapyPipeline:\n",
    "\n",
    "    def open_spider(self, spider):\n",
    "        self.db = get_mongodb_client()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbcaec80",
   "metadata": {},
   "source": [
    "### Below there are two options to send the data to MongoDB. <br> I. sending single data into the database then use first option."
   ]
  },
  {
   "cell_type": "raw",
   "id": "7dc8adea",
   "metadata": {},
   "source": [
    "def process_item(self, item, spider):\n",
    "        collection = self.db['quotes_tb']\n",
    "        collection.insert_one(dict(item))\n",
    "        return item"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11ead04",
   "metadata": {},
   "source": [
    "### II. sending the data in bulk or in batches then use second option."
   ]
  },
  {
   "cell_type": "raw",
   "id": "a9b9870b",
   "metadata": {},
   "source": [
    "def process_item(self, item, spider):\n",
    "        items_to_insert = []\n",
    "        if item:\n",
    "            items_to_insert.append(dict(item))\n",
    "        if len(items_to_insert) >= 10:  # Adjust batch size as needed\n",
    "            collection = self.db['quotes_tb']\n",
    "            collection.insert_many(items_to_insert)\n",
    "            items_to_insert = []\n",
    "            print(items_to_insert)\n",
    "        else:\n",
    "            logging.warning(\"Skipping empty item\")\n",
    "        return item"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09adb0e",
   "metadata": {},
   "source": [
    "### 1. here in the first funtion we are importing the mongodb atlas credentials from a program created to store these credentials. i.e. mongodb_client.py. <br> 2. then we are processing the scraped data and storing it in the collection i.e. tables in the form of dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a4f89b",
   "metadata": {},
   "source": [
    "## mongodb_client.py"
   ]
  },
  {
   "cell_type": "raw",
   "id": "539e7fdb",
   "metadata": {},
   "source": [
    "import os\n",
    "from pymongo import MongoClient\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()  \n",
    "\n",
    "def get_mongodb_client():\n",
    "    mongo_url = os.getenv(\"MONGODB_URL\")\n",
    "    mongo_db = os.getenv(\"MONGODB_DATABASE\")\n",
    "    client = MongoClient(mongo_url)\n",
    "    return client[mongo_db]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f305f0",
   "metadata": {},
   "source": [
    "### here we have created a function to call for establishing the connection with mongodb atlas using the database name, mongodb connection url."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fa9071",
   "metadata": {},
   "source": [
    "## .env file"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a2605ff2",
   "metadata": {},
   "source": [
    "MONGODB_URL = mongodb+srv://<username>:<password>@cluster0.17pa9ai.mongodb.net/?retryWrites=true&w=majority\n",
    "MONGODB_DATABASE = <database name>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deba9f90",
   "metadata": {},
   "source": [
    "### in the .env file, we have stored the mongodb database name and mongodb atlas connection url."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef6b411",
   "metadata": {},
   "source": [
    "## 2. by running the scapper program i.e. scrapyquotes.py file it will first scrape the data then store the data in dictionary format and send it to the mongodb database."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
