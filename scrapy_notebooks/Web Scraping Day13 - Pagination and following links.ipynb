{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b27c91d",
   "metadata": {},
   "source": [
    "# PAGINATION AND FOLLOWING LINKS IN WEB SCRAPING & CRAWLING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afe7084",
   "metadata": {},
   "source": [
    "### By - Nilutpal Das"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06d337e",
   "metadata": {},
   "source": [
    "### In most of the websites information that cannot be displayed or stored in the first page. so they adopted multiple pages to store or display the data.  <br> When we start performing web scraping or web crawling these pages comes as a challenge for scrapper or crawler. to overcome this challenge we use two different methods 'following links' or 'pagination'. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f14a626",
   "metadata": {},
   "source": [
    "### Method 1 - Following links, this method comes when at the end of every webpage has a 'next' button to go to next page. here by clicking inspect button you will get main 'li' tag with class next and inside it has 'a' tag which has attribute of href. this 'href' attribute stores the next page link which will navigate the browser to the next page."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564d83fa",
   "metadata": {},
   "source": [
    "## scrapyquotes.py"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bcbe7751",
   "metadata": {},
   "source": [
    "# CREATING AND RUNNING THE FIRST SPIDER CRAWLER\n",
    "\n",
    "# importing the library\n",
    "import scrapy\n",
    "from ..items import WebscrapyItem\n",
    "\n",
    "# creating class \n",
    "class QuoteSpider(scrapy.Spider):\n",
    "    # creating variable to store the name of the webscrapper\n",
    "    name = 'quotes'\n",
    "    # creating variable to store page number\n",
    "    page_number = 2\n",
    "    # creating variable to store the url link of page 1\n",
    "    start_urls = ['https://quotes.toscrape.com/']\n",
    "    \n",
    "\n",
    "    # defining a function name parse\n",
    "    def parse(self, response):\n",
    "        \n",
    "        # creating an instance variable for WebscrapyItem class\n",
    "        items = WebscrapyItem()\n",
    "        \n",
    "        # creating a new variable to extract the quotes using css selectors\n",
    "        all_div_quotes = response.css('div.quote') # we are not using .extract() function\n",
    "        # we are just locating the div tag and inside it has span tag along with text as class\n",
    "        \n",
    "        for quotes in all_div_quotes:\n",
    "            \n",
    "            # creating a new variable for extracting quotes\n",
    "            quote = quotes.css('span.text::text').extract()\n",
    "            # creating a new variable for extracting authors\n",
    "            author = quotes.css('.author::text').extract()\n",
    "            # creating a new variable for extracting tags\n",
    "            tag = quotes.css('.tag::text').extract()\n",
    "            \n",
    "            # using the blueprint of the class WebscrapyItem() \n",
    "            # storing the data in the proper designated containers \n",
    "            items['quote'] = quote\n",
    "            items['author'] = author\n",
    "            items['tag'] = tag\n",
    "\n",
    "            # returning the output using yield keyword\n",
    "            yield items\n",
    "            \n",
    "       next_page = response.css('li.next a::attr(href)').get()\n",
    "\n",
    "        if next_page1 is not None:\n",
    "            yield response.follow(next_page, callback=self.parse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4e8139",
   "metadata": {},
   "source": [
    "### Method 2 - Pagination, this method comes when the webpages does not contains any button for going to next page instead it contains pagination. here we are using the page number mentioned in the URL i.e. page number=1 or page-1. through the page number from the URL we scrape one page then we move forward with another page till we reach at the last page."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afd4936",
   "metadata": {},
   "source": [
    "## scrapyquotes.py"
   ]
  },
  {
   "cell_type": "raw",
   "id": "05b70628",
   "metadata": {},
   "source": [
    "# CREATING AND RUNNING THE FIRST SPIDER CRAWLER\n",
    "\n",
    "# importing the library\n",
    "import scrapy\n",
    "from ..items import WebscrapyItem\n",
    "\n",
    "# creating class \n",
    "class QuoteSpider(scrapy.Spider):\n",
    "    # creating variable to store the name of the webscrapper\n",
    "    name = 'quotes'\n",
    "    # creating variable to store page number\n",
    "    page_number = 2\n",
    "    # creating variable to store the url link of page 1\n",
    "    start_urls = ['https://quotes.toscrape.com/page/1/']\n",
    "    \n",
    "\n",
    "    # defining a function name parse\n",
    "    def parse(self, response):\n",
    "        \n",
    "        # creating an instance variable for WebscrapyItem class\n",
    "        items = WebscrapyItem()\n",
    "        \n",
    "        # creating a new variable to extract the quotes using css selectors\n",
    "        all_div_quotes = response.css('div.quote') # we are not using .extract() function\n",
    "        # we are just locating the div tag and inside it has span tag along with text as class\n",
    "        \n",
    "        for quotes in all_div_quotes:\n",
    "            \n",
    "            # creating a new variable for extracting quotes\n",
    "            quote = quotes.css('span.text::text').extract()\n",
    "            # creating a new variable for extracting authors\n",
    "            author = quotes.css('.author::text').extract()\n",
    "            # creating a new variable for extracting tags\n",
    "            tag = quotes.css('.tag::text').extract()\n",
    "            \n",
    "            # using the blueprint of the class WebscrapyItem() \n",
    "            # storing the data in the proper designated containers \n",
    "            items['quote'] = quote\n",
    "            items['author'] = author\n",
    "            items['tag'] = tag\n",
    "\n",
    "            # returning the output using yield keyword\n",
    "            yield items\n",
    "\n",
    "        \n",
    "        # creating a new variable to go to the next page\n",
    "        next_page = 'https://quotes.toscrape.com/page/' + str(QuoteSpider.page_number) + '/'\n",
    "\n",
    "        # if the next page is not less than 11\n",
    "        if QuoteSpider.page_number < 11:\n",
    "        \n",
    "            # then add number 1 to the current page number\n",
    "            QuoteSpider.page_number += 1\n",
    "            \n",
    "            # using yield keyword and follow keyword\n",
    "            # go to the next page and starts the parse function to scrape data\n",
    "            # and this will continue till the last page\n",
    "            yield response.follow(next_page, callback=self.parse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802debc7",
   "metadata": {},
   "source": [
    "### from starting we are scraping the first page that contains only 20 quotes. which can be sent to the mongodb using insert_one function. <br> now from here as we are scraping entire quotes from all 10 pages, to store such huge data in mongodb we have to using the insert_many function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc87962",
   "metadata": {},
   "source": [
    "### here based on your preference you select the method after that if you run the code you will get the scraped data. Also you can store these data anywhere you wish to store."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce0ce95",
   "metadata": {},
   "source": [
    "## NOTE : code related to 'insert_many' function has been discussed in the 'Web Scraping Day12 - Storing the data in MongoDB' notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
