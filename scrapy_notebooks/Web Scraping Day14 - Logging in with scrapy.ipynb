{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "365fba29",
   "metadata": {},
   "source": [
    "# LOGGING IN WITH SCRAPY FORMREQUEST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1f7ecd",
   "metadata": {},
   "source": [
    "### By - Nilutpal Das"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7441feb0",
   "metadata": {},
   "source": [
    "### When you plan to scrape data from a website, often times websites tries to restrict you from getting access to the website content using the help of logging page. they redirect you to a login page where you have to first give your user credentials then after that you get your access to the website unless the data is restricted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3127b2",
   "metadata": {},
   "source": [
    "### before scraping first we have to understand how the website is interacting based on the webpages. if you see when you are in the main page the URL of the website e.g. \"https://quotes.toscrape.com/\" can be seen.  <br> then if you click on logging page the website URL gets changed and '/login' added at the end \"https://quotes.toscrape.com/login\". after logged in the website URL becomes as former one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7a8b50",
   "metadata": {},
   "source": [
    "### to check how the logging in procedure works which will help you at scraping, first you have to first go to the login page then click inspect and then go to the network section. now if you try to login in to the webpage, in the network section you will see some updates are there. there you will see requests first login request, then website request and some other requests too."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff58994",
   "metadata": {},
   "source": [
    "### go to the login request, there go at the end of the list you will see three important data : first csrf_token, second username and last password.\n",
    "**remember, these lines will be used afterwards**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed07b97",
   "metadata": {},
   "source": [
    "### now let's go to the code of scrapyquotes.py file, we have to do some lil modifications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432eafb5",
   "metadata": {},
   "source": [
    "## scrapyquotes.py"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a35f71ce",
   "metadata": {},
   "source": [
    "# CREATING AND RUNNING THE FIRST SPIDER CRAWLER\n",
    "\n",
    "# importing the library\n",
    "import scrapy\n",
    "from ..items import WebscrapyItem\n",
    "from scrapy.http import FormRequest\n",
    "from scrapy.utils.response import open_in_browser\n",
    "\n",
    "# creating class \n",
    "class QuoteSpider(scrapy.Spider):\n",
    "    # creating variable to store the name of the webscrapper\n",
    "    name = 'quotes'\n",
    "    # creating variable to store page number\n",
    "    page_number = 2\n",
    "    # creating variable to store the url link of page 1\n",
    "    start_urls = ['https://quotes.toscrape.com/login']\n",
    "    \n",
    "\n",
    "    # defining a function name parse\n",
    "    def parse(self, response):\n",
    "        \n",
    "        token = response.css('form input::attr(value)').extract_first()\n",
    "        return FormRequest.from_response(response=response, formdata={\n",
    "            'csrf_token' : token,\n",
    "            'username' : 'webscrapingbyscrapy',\n",
    "            'password' : 'helloworld'\n",
    "        }, callback= self.start_scraping)\n",
    "    \n",
    "    def start_scraping(self,response):\n",
    "        open_in_browser(response=response)\n",
    "        \n",
    "        # creating an instance variable for WebscrapyItem class\n",
    "        items = WebscrapyItem()\n",
    "        \n",
    "        # creating a new variable to extract the quotes using css selectors\n",
    "        all_div_quotes = response.css('div.quote') # we are not using .extract() function\n",
    "        # we are just locating the div tag and inside it has span tag along with text as class\n",
    "        \n",
    "        for quotes in all_div_quotes:\n",
    "            \n",
    "            # creating a new variable for extracting quotes\n",
    "            quote = quotes.css('span.text::text').extract()\n",
    "            # creating a new variable for extracting authors\n",
    "            author = quotes.css('.author::text').extract()\n",
    "            # creating a new variable for extracting tags\n",
    "            tag = quotes.css('.tag::text').extract()\n",
    "            \n",
    "            # using the blueprint of the class WebscrapyItem() \n",
    "            # storing the data in the proper designated containers \n",
    "            items['quote'] = quote\n",
    "            items['author'] = author\n",
    "            items['tag'] = tag\n",
    "\n",
    "            # returning the output using yield keyword\n",
    "            yield items"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0758468d",
   "metadata": {},
   "source": [
    "### 1) here at first we have imported two new packages, formrequest to going to fetch data of formdata and open_in_browser is used to open the page that is going to be scraped. <br> 2) second in the start_urls variable the link is being changed from first page of the website to the login page. <br> 3) third in the parse function, we have introduced new variable token which uses css selectors. it started locating using csrf_token, username and password as key and select the section where these formdata is stored. and forward it to the start_scraping function. <4> at last we have used the previous parse function code in the start_scraping function to scrape the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae4dfab",
   "metadata": {},
   "source": [
    "### by running this code it first login into the website using the username and password then open the page in the website and at last it starts scraping the data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
