{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87b84631",
   "metadata": {},
   "source": [
    "# BYPASS RESTRICTIONS USING USER AGENTS AND PROXIES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2955b02",
   "metadata": {},
   "source": [
    "### By - Nilutpal Das"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6152c846",
   "metadata": {},
   "source": [
    "### Scraping a real websites in real time is very tricky because let's be clear no websites wants their website's data to be scraped and crawled. To do this they put restrictions or sometimes complete ban on some devices or machines if they visit on their websites. So we have to do the scraping part very responsibly and ethically. to avoid getting ban we have to BYPASS RESTRICTIONS. bypassing can be done using two method one is using user agents and other is  using proxies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c3a8f5",
   "metadata": {},
   "source": [
    "# 1. Bypass Restrictions using User Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4e48bd",
   "metadata": {},
   "source": [
    "### Whenever any browser be it chrome, mozilla or safari visits a website. the websites asks the browser its identity and that identity is called User Agents and every browser have this User Agents. \n",
    "### In technical term it is a string of characters that your web browser sends in the “User-Agent” header for your HTTP requests to the website."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caee282e",
   "metadata": {},
   "source": [
    "### Bypassing the restrictions using user agents can also be of two types : <br> 1. replace your user agents with the google crawlers user agents which are allowed by the websites i.e. google bot. websites which are online have to allow the google bots to crawl because without that their websites cannot be in the user's google search results. this way we tricks the websites that actually google is crawling not a web scraper.  \n",
    "**\"Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)\"**\n",
    "### 2. keep rotating the user agents, if the websites finds out the web crawler in their websites so just switch from real user agents to fake user agents. this way you are keep on rotating and tricking the websites that numerous browsers are visiting the websites rather than crawlers. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bddcb6d9",
   "metadata": {},
   "source": [
    "# 2. Bypass Restrictions using Proxies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5649e98",
   "metadata": {},
   "source": [
    "### Bypassing using proxies starts with IP addresses. the IP addresses is the address of your computer which get shared whenever your browser visits a websites it first checks the IP address and then allowed the browser."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86953df4",
   "metadata": {},
   "source": [
    "### the general meaning of the word 'proxy' means that the authority to represent someone else. exactly similar concept is being used in bypassing the restrictions. here we are using the IP addresses of someone else in place of our IP address. these IP address are unique and also using it in a rotation just like the user agents so that it keeps on changing after a period of time whenever we sends a requests to the website."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
