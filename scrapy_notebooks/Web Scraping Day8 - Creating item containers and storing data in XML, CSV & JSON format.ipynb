{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "faf14542",
   "metadata": {},
   "source": [
    "# CREATING ITEM CONTAINERS AND STORING DATA IN XML, JSON & CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01a4878",
   "metadata": {},
   "source": [
    "### By - Nilutpal Das"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e577dea",
   "metadata": {},
   "source": [
    "## NOTE : the process shown in this section is built to be run in dedicated IDE also inside a specific folder discussed in the previous notebooks.\n",
    "\n",
    "\n",
    "## NOTE : storing the scraped data at first place is very tricky as because of lack of structure.\n",
    "\n",
    "# STEP 1 : ITEM CONTAINERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e4edf53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\Data Science\\\\jupyter-notebooks\\\\data science\\\\web_scraping'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7a62fef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Data Science\\jupyter-notebooks\\data science\\web_scraping\\webscrapy\n"
     ]
    }
   ],
   "source": [
    "cd webscrapy/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d496296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive D is Data\n",
      " Volume Serial Number is EC2B-97AB\n",
      "\n",
      " Directory of D:\\Data Science\\jupyter-notebooks\\data science\\web_scraping\\webscrapy\n",
      "\n",
      "27-12-2023  08.04 PM    <DIR>          .\n",
      "27-12-2023  08.04 PM    <DIR>          ..\n",
      "27-12-2023  08.04 PM               272 scrapy.cfg\n",
      "28-12-2023  09.54 AM    <DIR>          webscrapy\n",
      "               1 File(s)            272 bytes\n",
      "               3 Dir(s)  418,620,162,048 bytes free\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b8cbd8f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Data Science\\jupyter-notebooks\\data science\\web_scraping\\webscrapy\\webscrapy\n"
     ]
    }
   ],
   "source": [
    "cd webscrapy/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13a2bcf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive D is Data\n",
      " Volume Serial Number is EC2B-97AB\n",
      "\n",
      " Directory of D:\\Data Science\\jupyter-notebooks\\data science\\web_scraping\\webscrapy\\webscrapy\n",
      "\n",
      "28-12-2023  09.54 AM    <DIR>          .\n",
      "28-12-2023  09.54 AM    <DIR>          ..\n",
      "27-12-2023  07.18 PM                 0 __init__.py\n",
      "07-01-2024  07.20 PM    <DIR>          __pycache__\n",
      "07-01-2024  07.09 PM               453 items.py\n",
      "27-12-2023  08.04 PM             3,757 middlewares.py\n",
      "27-12-2023  08.04 PM               376 pipelines.py\n",
      "27-12-2023  08.04 PM             3,410 settings.py\n",
      "28-12-2023  09.54 AM    <DIR>          spiders\n",
      "               5 File(s)          7,996 bytes\n",
      "               4 Dir(s)  418,620,157,952 bytes free\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e866cbd",
   "metadata": {},
   "source": [
    "## head over to the items.py file in the webscrapy folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a261dc",
   "metadata": {},
   "source": [
    "# items.py \n",
    "\n",
    "### 1. in this file there is already a created class WebscrapyItem() in items.py file <br> 2. after creating all the fields related to our extracting data."
   ]
  },
  {
   "cell_type": "raw",
   "id": "93a44db2",
   "metadata": {},
   "source": [
    "# Define here the models for your scraped items\n",
    "\n",
    "# See documentation in:\n",
    "# https://docs.scrapy.org/en/latest/topics/items.html\n",
    "\n",
    "# process structures of storing the extracted data\n",
    "# extracted data -> temporary containers (items) -> stored in database\n",
    "\n",
    "import scrapy\n",
    "\n",
    "class WebscrapyItem(scrapy.Item):\n",
    "    # define the fields for your item here like:\n",
    "    quote = scrapy.Field()\n",
    "    author = scrapy.Field()\n",
    "    tag = scrapy.Field()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9ce192",
   "metadata": {},
   "source": [
    "## then go to the scrapyquotes.py inside the spider folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "afd73ec9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive D is Data\n",
      " Volume Serial Number is EC2B-97AB\n",
      "\n",
      " Directory of D:\\Data Science\\jupyter-notebooks\\data science\\web_scraping\\webscrapy\\webscrapy\n",
      "\n",
      "28-12-2023  09.54 AM    <DIR>          .\n",
      "28-12-2023  09.54 AM    <DIR>          ..\n",
      "27-12-2023  07.18 PM                 0 __init__.py\n",
      "07-01-2024  08.06 PM    <DIR>          __pycache__\n",
      "07-01-2024  07.32 PM               445 items.py\n",
      "27-12-2023  08.04 PM             3,757 middlewares.py\n",
      "27-12-2023  08.04 PM               376 pipelines.py\n",
      "27-12-2023  08.04 PM             3,410 settings.py\n",
      "28-12-2023  09.54 AM    <DIR>          spiders\n",
      "               5 File(s)          7,988 bytes\n",
      "               4 Dir(s)  418,620,125,184 bytes free\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0a6fe9b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Data Science\\jupyter-notebooks\\data science\\web_scraping\\webscrapy\\webscrapy\\spiders\n"
     ]
    }
   ],
   "source": [
    "cd spiders/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "45235d49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive D is Data\n",
      " Volume Serial Number is EC2B-97AB\n",
      "\n",
      " Directory of D:\\Data Science\\jupyter-notebooks\\data science\\web_scraping\\webscrapy\\webscrapy\\spiders\n",
      "\n",
      "28-12-2023  09.54 AM    <DIR>          .\n",
      "28-12-2023  09.54 AM    <DIR>          ..\n",
      "28-12-2023  07.30 PM               161 __init__.py\n",
      "07-01-2024  08.06 PM    <DIR>          __pycache__\n",
      "07-01-2024  07.52 PM             1,555 scrapyquotes.py\n",
      "               2 File(s)          1,716 bytes\n",
      "               3 Dir(s)  418,620,125,184 bytes free\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09502b3d",
   "metadata": {},
   "source": [
    "# scrapyquotes.py\n",
    "\n",
    "### 1. first import the class from items.py <br> 2. creating instance variable <br> 3. storing the data in the containers <br> 6. returning the data in structured fromat"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b7550db5",
   "metadata": {},
   "source": [
    "# CREATING AND RUNNING THE FIRST SPIDER CRAWLER\n",
    "\n",
    "# importing the library\n",
    "import scrapy\n",
    "from ..items import WebscrapyItem\n",
    "\n",
    "# creating class \n",
    "class QuoteSpider(scrapy.Spider):\n",
    "    # creating a name variable \n",
    "    name = 'quotes'\n",
    "    # creating start_urls variable to store the url link\n",
    "    start_urls = ['https://quotes.toscrape.com/']\n",
    "    \n",
    "\n",
    "    # defining a function parse\n",
    "    def parse(self, response):\n",
    "        \n",
    "        # creating an instance variable for WebscrapyItem class\n",
    "        items = WebscrapyItem()\n",
    "        \n",
    "        # creating a new variable to extract the quotes using css selectors\n",
    "        all_div_quotes = response.css('div.quote') # we are not using .extract() function\n",
    "        # we are just locating the div tag and inside it has span tag along with text as class\n",
    "        \n",
    "        for quotes in all_div_quotes:\n",
    "            \n",
    "            # creating a new variable for extracting quotes\n",
    "            quote = quotes.css('span.text::text').extract()\n",
    "            # creating a new variable for extracting authors\n",
    "            author = quotes.css('.author::text').extract()\n",
    "            # creating a new variable for extracting tags\n",
    "            tag = quotes.css('.tag::text').extract()\n",
    "            \n",
    "            # using the blueprint of the class WebscrapyItem() \n",
    "            # storing the data in the proper designated containers\n",
    "            items['quote'] = quote\n",
    "            items['author'] = author\n",
    "            items['tag'] = tag\n",
    "\n",
    "            # returning the output using yield keyword\n",
    "            yield items"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ba7fe6",
   "metadata": {},
   "source": [
    "# output\n",
    "\n",
    "### the output is in complete structured format and it is suitable for storing it in a database"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1cdb6a23",
   "metadata": {},
   "source": [
    "{'author': ['Albert Einstein'],\n",
    " 'quote': ['“The world as we have created it is a process of our thinking. It '\n",
    "           'cannot be changed without changing our thinking.”'],\n",
    " 'tag': ['change', 'deep-thoughts', 'thinking', 'world']}\n",
    "\n",
    "{'author': ['J.K. Rowling'],\n",
    " 'quote': ['“It is our choices, Harry, that show what we truly are, far more '\n",
    "           'than our abilities.”'],\n",
    " 'tag': ['abilities', 'choices']}\n",
    "\n",
    "{'author': ['Albert Einstein'],\n",
    " 'quote': ['“There are only two ways to live your life. One is as though '\n",
    "           'nothing is a miracle. The other is as though everything is a '\n",
    "           'miracle.”'],\n",
    " 'tag': ['inspirational', 'life', 'live', 'miracle', 'miracles']}\n",
    "\n",
    "{'author': ['Jane Austen'],\n",
    " 'quote': ['“The person, be it gentleman or lady, who has not pleasure in a '\n",
    "           'good novel, must be intolerably stupid.”'],\n",
    " 'tag': ['aliteracy', 'books', 'classic', 'humor']}\n",
    "\n",
    "{'author': ['Marilyn Monroe'],\n",
    " 'quote': [\"“Imperfection is beauty, madness is genius and it's better to be \"\n",
    "           'absolutely ridiculous than absolutely boring.”'],\n",
    " 'tag': ['be-yourself', 'inspirational']}\n",
    "\n",
    "{'author': ['Albert Einstein'],\n",
    " 'quote': ['“Try not to become a man of success. Rather become a man of '\n",
    "           'value.”'],\n",
    " 'tag': ['adulthood', 'success', 'value']}\n",
    "\n",
    "{'author': ['André Gide'],\n",
    " 'quote': ['“It is better to be hated for what you are than to be loved for '\n",
    "           'what you are not.”'],\n",
    " 'tag': ['life', 'love']}\n",
    "\n",
    "{'author': ['Thomas A. Edison'],\n",
    " 'quote': [\"“I have not failed. I've just found 10,000 ways that won't work.”\"],\n",
    " 'tag': ['edison', 'failure', 'inspirational', 'paraphrased']}\n",
    "\n",
    "{'author': ['Eleanor Roosevelt'],\n",
    " 'quote': ['“A woman is like a tea bag; you never know how strong it is until '\n",
    "           \"it's in hot water.”\"],\n",
    " 'tag': ['misattributed-eleanor-roosevelt']}\n",
    "\n",
    "{'author': ['Steve Martin'],\n",
    " 'quote': ['“A day without sunshine is like, you know, night.”'],\n",
    " 'tag': ['humor', 'obvious', 'simile']}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27418e2",
   "metadata": {},
   "source": [
    "# STEP 2 : STORING DATA IN XML, JSON & CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1aa0a5",
   "metadata": {},
   "source": [
    "## run the command in the terminal as per the file format you want \n",
    "**scrapy crawl quotes -o items.csv** <br>\n",
    "**scrapy crawl quotes -o items.json** <br>\n",
    "**scrapy crawl quotes -o items.xml**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee45629a",
   "metadata": {},
   "source": [
    "# data is stored in the desired format"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
