{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "239d6d44",
   "metadata": {},
   "source": [
    "# WEB SCRAPING AMAZON.IN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452b9a08",
   "metadata": {},
   "source": [
    "### By - Nilutpal Das"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd103eb",
   "metadata": {},
   "source": [
    "## NOTE : before jumping into the scraping part. here there are important note to remember.\n",
    "### 1. Respect website's terms of service: Use scraping responsibly and ethically. <br> 2. Avoid overwheling websites with too many requests: Be mindful of their resources. <br> 3. Handle legal and ethical considerations: Respect data privacy and copyright laws."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60938f70",
   "metadata": {},
   "source": [
    "### Scraping a real websites in real time is very tricky because let's be clear no websites wants their website's data to be scraped and crawled. To do this they put restrictions or sometimes complete ban on some devices or machines if they visit on their websites. So we have to do the scraping part very responsibly and ethically. Now let's come to the scraping part, first we create a scrapy folder and inside it run the scrapy. this will create all the necessary files and folders needed for scrapy. also if you run the genspider command it will also create a demo spider. here below mentioned code is the modified code of demo spider for scraping."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455f7f15",
   "metadata": {},
   "source": [
    "### amazonspider.py"
   ]
  },
  {
   "cell_type": "raw",
   "id": "17b9f0ab",
   "metadata": {},
   "source": [
    "# importing libraries\n",
    "import scrapy\n",
    "from ..items import AmazonscrapyItem\n",
    "\n",
    "\n",
    "class AmazonSpider(scrapy.Spider):\n",
    "    \n",
    "    # creating variable to store the name of the webscrapper\n",
    "    name = \"amazon\"\n",
    "    \n",
    "    # creating variable to store the url link of page 1\n",
    "    start_urls = [\"https://www.amazon.in/s?i=computers&rh=n%3A7198569031&fs=true&qid=1706417538&ref=sr_pg_1\"]\n",
    "\n",
    "    def parse(self, response):\n",
    "        \n",
    "        # creating an instance variable for WebscrapyItem class\n",
    "        items = AmazonscrapyItem()\n",
    "        \n",
    "        # creating a new variable to extract the quotes using css selectors\n",
    "        product_name = response.css('.a-color-base.a-text-normal').css('::text').extract()\n",
    "        product_rating = response.css('.aok-align-bottom').css('::text').extract()\n",
    "        product_price = response.css('.a-price-whole').css('::text').extract()\n",
    "        product_imagelink = response.css('.s-image::attr(src)').extract()\n",
    "        \n",
    "        \n",
    "        # using the blueprint of the class WebscrapyItem() \n",
    "        # storing the data in the proper designated containers \n",
    "        items['product_name'] = product_name\n",
    "        items['product_rating'] = product_rating\n",
    "        items['product_price'] = product_price\n",
    "        items['product_imagelink'] = product_imagelink\n",
    "        \n",
    "        # returning the output using yield keyword\n",
    "        yield items"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4883399",
   "metadata": {},
   "source": [
    "### 1. first import the necessary libraries, such as scrapy and AmazonscrapyItem. <br> 2. then in the start_urls add https://www.amazon.in/' or the page from that you are going to scrape. in my case i am going to scrape list of gaming laptops. <br> 3. in the parse function first create the amazonscrapyitem variable instance. <br> after that make a list of things that you are going to scrape. for me product_name, product_rating, product_price and product_imagelink using css selectors. you can use the inspect option or use selectorgadget chrome extension for css selectors.<br> 4. create a containers for storing and formatting scraped data so that all the scraped data should be in dictionary format. <br> 5. at the end use the yield keyword for generating the results of web scraping."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c603e4c",
   "metadata": {},
   "source": [
    "### To store the data we need, our items.py should be modified as per our requirements. so modifying the items based on the key and values of scraped data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca30cce3",
   "metadata": {},
   "source": [
    "### items.py"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ff0ee830",
   "metadata": {},
   "source": [
    "# Define here the models for your scraped items\n",
    "#\n",
    "# See documentation in:\n",
    "# https://docs.scrapy.org/en/latest/topics/items.html\n",
    "\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class AmazonscrapyItem(scrapy.Item):\n",
    "    # define the fields for your item here like:\n",
    "    product_name = scrapy.Field()\n",
    "    product_rating = scrapy.Field()\n",
    "    product_price = scrapy.Field()\n",
    "    product_imagelink = scrapy.Field()\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f3ab37",
   "metadata": {},
   "source": [
    "### here inside the class just add all the fields mentioned in the scrapy code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bd57de",
   "metadata": {},
   "source": [
    "### Now, run the program you will get all the scraped data. <br> if need some adjustment on the scraped data then do the modifcation needed. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c66ae09",
   "metadata": {},
   "source": [
    "### that's it you have sucessfully scraped a real website with scrapy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eaf2bdf",
   "metadata": {},
   "source": [
    "## NOTE : till here the web scraping is shown for the page 1. if your website have multiple pages then you can follow the pagination section of 'Web Scraping Day13 - Pagination and following links' notebook or you can also continue from this on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4816e149",
   "metadata": {},
   "source": [
    "## amazonspider.py"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6b87ec05",
   "metadata": {},
   "source": [
    "# importing libraries\n",
    "import scrapy\n",
    "from ..items import AmazonscrapyItem\n",
    "\n",
    "\n",
    "class AmazonSpider(scrapy.Spider):\n",
    "    # create a variable for name of webscrapper\n",
    "    name = \"amazon\"\n",
    "\n",
    "    # create a variable for the page number\n",
    "    page_number = 2\n",
    "\n",
    "    # create a variable for the webpage link\n",
    "    start_urls = [\n",
    "        \"https://www.amazon.in/s?i=computers&rh=n%3A7198569031&fs=true&qid=1706417538&ref=sr_pg_1\"\n",
    "    ]\n",
    "\n",
    "    def parse(self, response):\n",
    "\n",
    "        # creating an instance variable for WebscrapyItem class\n",
    "        items = AmazonscrapyItem()\n",
    "\n",
    "        # creating a new variable for extracting product_name\n",
    "        product_name = response.css(\n",
    "            '.a-color-base.a-text-normal').css('::text').extract()\n",
    "        # creating a new variable for extracting product_name\n",
    "        product_rating = response.css(\n",
    "            '.aok-align-bottom').css('::text').extract()\n",
    "        # creating a new variable for extracting product_name\n",
    "        product_price = response.css('.a-price-whole').css('::text').extract()\n",
    "        # creating a new variable for extracting product_name\n",
    "        product_imagelink = response.css('.s-image::attr(src)').extract()\n",
    "\n",
    "        # using the blueprint of the class WebscrapyItem()\n",
    "        # storing the data in the proper designated containers\n",
    "        items['product_name'] = product_name\n",
    "        items['product_rating'] = product_rating\n",
    "        items['product_price'] = product_price\n",
    "        items['product_imagelink'] = product_imagelink\n",
    "        \n",
    "        # returning the output using yield keyword\n",
    "        yield items\n",
    "        \n",
    "        # creating a new variable to go to the next page\n",
    "        next_page = 'https://www.amazon.in/s?i=computers&rh=n%3A7198569031&fs=true&' + \\\n",
    "            str(AmazonSpider.page_number) + '&qid=1706672969&ref=sr_pg_2'\n",
    "        \n",
    "        if AmazonSpider.page_number <= 4:\n",
    "        \n",
    "            # then add number 1 to the current page number\n",
    "            AmazonSpider.page_number += 1\n",
    "            \n",
    "            # using yield keyword and follow keyword\n",
    "            # go to the next page and starts the parse function to scrape data\n",
    "            # and this will continue till the last page\n",
    "            yield response.follow(next_page, callback=self.parse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b397c80",
   "metadata": {},
   "source": [
    "### 1. here first a new variable is created to store page number. <br> 2. then create a new variable inside the parse function to store the next page of the website. <br> 3. after that using if statement check the maximum number of web pages are present or not in the website. <br> 4. if it does not exceed the mentioned page number then move forward with one more page number. <br> 5. at the end after visiting each page scrape the data and then move on to next page."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18b9e5a",
   "metadata": {},
   "source": [
    "### by this all the pages have been scraped of gaming laptop section in amazon.in."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
