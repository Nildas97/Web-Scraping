{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab650982",
   "metadata": {},
   "source": [
    "# PIPELINES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6947b1ea",
   "metadata": {},
   "source": [
    "### By - Nilutpal Das"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86cbd6e7",
   "metadata": {},
   "source": [
    "# NOTE : the process shown in this section is built to be run in dedicated IDE also inside a specific folder discussed in the previous notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c021d7",
   "metadata": {},
   "source": [
    "### 1. the whole procedure of first scraping the data then creating temporary item containers and at last storing in the JSON, CSV, XML files is shown below.\n",
    "\n",
    "## process - (extract the data >> store in temp item containers >> storing in JSON, CSV, XML files)\n",
    "\n",
    "### 2. the process needs to be updated a lil bit by introducing the 'pipelines'.\n",
    "\n",
    "## updated process - (extract the data >> store in temp item containers >> pipelines >> storing in MySQL/MongoDB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c54e8d4",
   "metadata": {},
   "source": [
    "##  NOTE : the pipelines file has all the components that needs to be in the pipelines to run the scraping process smoothly, but still we have to enable the pipeline in the settings.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ce9e1a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\Data Science\\\\jupyter-notebooks\\\\data science\\\\web_scraping'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28eb7ae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Data Science\\jupyter-notebooks\\data science\\web_scraping\\webscrapy\n"
     ]
    }
   ],
   "source": [
    "cd webscrapy/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fdcd117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive D is Data\n",
      " Volume Serial Number is EC2B-97AB\n",
      "\n",
      " Directory of D:\\Data Science\\jupyter-notebooks\\data science\\web_scraping\\webscrapy\n",
      "\n",
      "07-01-2024  08.06 PM    <DIR>          .\n",
      "07-01-2024  08.06 PM    <DIR>          ..\n",
      "07-01-2024  08.07 PM             1,428 items.csv\n",
      "07-01-2024  08.06 PM             1,864 items.json\n",
      "07-01-2024  08.06 PM             2,697 items.xml\n",
      "27-12-2023  08.04 PM               272 scrapy.cfg\n",
      "28-12-2023  09.54 AM    <DIR>          webscrapy\n",
      "               4 File(s)          6,261 bytes\n",
      "               3 Dir(s)  418,620,112,896 bytes free\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1def6445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Data Science\\jupyter-notebooks\\data science\\web_scraping\\webscrapy\\webscrapy\n"
     ]
    }
   ],
   "source": [
    "cd webscrapy/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7924ed41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive D is Data\n",
      " Volume Serial Number is EC2B-97AB\n",
      "\n",
      " Directory of D:\\Data Science\\jupyter-notebooks\\data science\\web_scraping\\webscrapy\\webscrapy\n",
      "\n",
      "28-12-2023  09.54 AM    <DIR>          .\n",
      "28-12-2023  09.54 AM    <DIR>          ..\n",
      "27-12-2023  07.18 PM                 0 __init__.py\n",
      "07-01-2024  08.06 PM    <DIR>          __pycache__\n",
      "07-01-2024  07.32 PM               445 items.py\n",
      "27-12-2023  08.04 PM             3,757 middlewares.py\n",
      "27-12-2023  08.04 PM               376 pipelines.py\n",
      "27-12-2023  08.04 PM             3,410 settings.py\n",
      "28-12-2023  09.54 AM    <DIR>          spiders\n",
      "               5 File(s)          7,988 bytes\n",
      "               4 Dir(s)  418,620,108,800 bytes free\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db324b6e",
   "metadata": {},
   "source": [
    "## pipelines.py"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bb83afb0",
   "metadata": {},
   "source": [
    "# Define your item pipelines here\n",
    "#\n",
    "# Don't forget to add your pipeline to the ITEM_PIPELINES setting\n",
    "# See: https://docs.scrapy.org/en/latest/topics/item-pipeline.html\n",
    "\n",
    "\n",
    "# useful for handling different item types with a single interface\n",
    "from itemadapter import ItemAdapter\n",
    "\n",
    "\n",
    "class WebscrapyPipeline:\n",
    "    def process_item(self, item, spider):\n",
    "        return item"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb07043",
   "metadata": {},
   "source": [
    "## NOTE : here the 'item' in the function process inside WebscrapyPipeline stores all the items along with the data that are coming after running the program in scrapyquotes.py."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea30b0a",
   "metadata": {},
   "source": [
    "## settings.py"
   ]
  },
  {
   "cell_type": "raw",
   "id": "05645fd6",
   "metadata": {},
   "source": [
    "# Configure item pipelines\n",
    "# See https://docs.scrapy.org/en/latest/topics/item-pipeline.html\n",
    "ITEM_PIPELINES = {\n",
    "   \"webscrapy.pipelines.WebscrapyPipeline\": 300,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ccd834",
   "metadata": {},
   "source": [
    "### 1. here in the settings.py file, we have to enable this line : <br> ITEM_PIPELINES = {\"webscrapy.pipelines.WebscrapyPipeline\": 300}. <br> 2. 300 is the number given to the pipelines, lower the number higher will be priority given to the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99c8a39",
   "metadata": {},
   "source": [
    "## NOTE : after enabling in the settings.py file the pipelines will be activated and it will work internally. but if you want to check whether the pipelines is working or not, simply update the code mentioned below in the pipelines.py file. \n",
    "\n",
    "## add this line \"print(\"Pipeline is working successfully\")\" in the code to check everytime the pipeline is working or not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b78d33",
   "metadata": {},
   "source": [
    "## pipelines.py"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c7a762ec",
   "metadata": {},
   "source": [
    "# Define your item pipelines here\n",
    "#\n",
    "# Don't forget to add your pipeline to the ITEM_PIPELINES setting\n",
    "# See: https://docs.scrapy.org/en/latest/topics/item-pipeline.html\n",
    "\n",
    "\n",
    "# useful for handling different item types with a single interface\n",
    "from itemadapter import ItemAdapter\n",
    "\n",
    "\n",
    "class WebscrapyPipeline:\n",
    "    def process_item(self, item, spider):\n",
    "        \n",
    "        print(\"Pipeline is working successfully\")\n",
    "\n",
    "        return item"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76241e0e",
   "metadata": {},
   "source": [
    "## run this command in the terminal\n",
    "**scrapy crawl quotes**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d598176d",
   "metadata": {},
   "source": [
    "## output"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0c598b21",
   "metadata": {},
   "source": [
    "Pipeline is working successfully\n",
    "2024-01-07 22:38:54 [scrapy.core.scraper] DEBUG: Scraped from <200 https://quotes.toscrape.com/>\n",
    "{'author': ['Albert Einstein'],\n",
    " 'quote': ['“The world as we have created it is a process of our thinking. It '\n",
    "           'cannot be changed without changing our thinking.”'],\n",
    " 'tag': ['change', 'deep-thoughts', 'thinking', 'world']}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
